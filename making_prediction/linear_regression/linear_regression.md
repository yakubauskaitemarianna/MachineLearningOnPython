# Линейная регрессия

Рассмотрим две непрерывные переменные ``` x = (x_1, x_2, ... , x_n)  y = (y_1, y_2, ... , y_n) ```.

Разместим две точки на двумерном [графике рассеяния](https://ru.wikipedia.org/wiki/%D0%94%D0%B8%D0%B0%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B0_%D1%80%D0%B0%D1%81%D1%81%D0%B5%D1%8F%D0%BD%D0%B8%D1%8F)
 и скажем, что мы имеем линейное соотношение, если данные аппроксимируются прямой линией. Если мы полагаем, что ```y``` зависит от ```x```, причем изменения в ```y``` вызываются именно изменениями в ```x```, мы можем опеределять линию регресии (регрессия ```y``` на ```x```), которая лучше всего описывает прямолинейное соотношение между этими двумя линейными переменными. 
 
 ## Простая (парная) линейная регрессия
 
 [Y = a + bx](http://statistica.ru/upload/medialibrary/teoria-lineinoi-regressii/ris1.PNG)
 
 
 ```x``` - независимая переменная, предиктор
 
 
 ```Y``` - зависимая переменная или переменная отклика. Это значение, которое мы ожидаем для ```y``` в среднем, если мы знаем величину ```x```, то это "предсказанное значение ```y```".
 
 
 ```a``` - свободный член (пересечение) линии оценки, значение ```Y```, когда ```x=0```
 
 
 ```b``` - угловой коэффициент, [градиент](http://matica.org.ua/metodichki-i-knigi-po-matematike/metody-optimizatcii-nekrasova-m-g/5-6-proizvodnaia-po-napravleniiu-gradient-linii-urovnia-funktcii) оцененной линии, представляет собой величину, на которую ```Y``` увеличивается в среднем, если ```x``` увеличивается на единицу.
 
 
 ### Метод наименьших квадратов 

Мы выполняем регрессионный анализ, используя выборку наблюдений, где ```a``` и ```b``` – выборочные оценки истинных (генеральных) параметров, ```α``` и ```β``` , которые определяют линию линейной регрессии в популяции (генеральной совокупности).
Наиболее простым методом определения коэффициентов ```a``` и ```b``` является метод наименьших квадратов [(МНК)](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%BD%D0%B0%D0%B8%D0%BC%D0%B5%D0%BD%D1%8C%D1%88%D0%B8%D1%85_%D0%BA%D0%B2%D0%B0%D0%B4%D1%80%D0%B0%D1%82%D0%BE%D0%B2).
Подгонка оценивается, рассматривая остатки (вертикальное расстояние каждой точки от линии, например, остаток = наблюдаемому ```y``` – предсказанный ```y```. [График](http://statistica.ru/upload/medialibrary/teoria-lineinoi-regressii/ris2.PNG)


Итак, для каждой наблюдаемой величины  остаток равен разнице  и соответствующего предсказанного  Каждый остаток может быть положительным или отрицательным.
Можно использовать остатки для проверки следующих предположений, лежащих в основе линейной регрессии:

- Между ```x``` и ```y``` существует линейное соотношение: для любых пар ```(x;y)``` данные должны аппроксимировать прямую линию; 
- Остатки нормально распределены с нулевым средним значением;
- Остатки имеют одну и ту же вариабельность (постоянную дисперсию) для всех предсказанных величин ```y```. Если нанести остатки против предсказанных величин ```Y``` от ```y``` мы должны наблюдать случайное рассеяние точек. Если график рассеяния остатков увеличивается или уменьшается с увеличением ```Y``` то это допущение не выполняется;
- Если допущения линейности, нормальности и/или постоянной дисперсии сомнительны, мы можем преобразовать ```x``` или  и ```y``` рассчитать новую линию регрессии, для которой эти допущения удовлетворяются (например, использовать логарифмическое преобразование или др.).

### Гипотеза линейной регрессии

При построении линейной регрессии проверяется нулевая гипотеза о том, что генеральный угловой коэффициент линии регрессии β равен нулю.

Если угловой коэффициент линии равен нулю, между  и  нет линейного соотношения: изменение ```x``` не влияет на ```y```. Для тестирования нулевой гипотезы о том, что истинный угловой коэффициент β равен 0 можно воспользоваться  алгоритмом :

Вычислить статистику критерия, равную отношению ```b/SE(b)~t(n-2)```, которая подчиняется t-распределенею с (n-2) степенями свободы, где SE(b) - стандартная ошибка коэффициента b. 
![alt text](http://statistica.ru/upload/medialibrary/provedenie-analiza-lineinoy-regressii/46.GIF)
