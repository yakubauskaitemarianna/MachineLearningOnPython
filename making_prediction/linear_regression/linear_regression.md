# Линейная регрессия

Рассмотрим две непрерывные переменные ``` x = (x_1, x_2, ... , x_n)  y = (y_1, y_2, ... , y_n) ```.

Разместим две точки на двумерном [графике рассеяния](https://ru.wikipedia.org/wiki/%D0%94%D0%B8%D0%B0%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B0_%D1%80%D0%B0%D1%81%D1%81%D0%B5%D1%8F%D0%BD%D0%B8%D1%8F)
 и скажем, что мы имеем линейное соотношение, если данные аппроксимируются прямой линией. Если мы полагаем, что ```y``` зависит от ```x```, причем изменения в ```y``` вызываются именно изменениями в ```x```, мы можем опеределять линию регресии (регрессия ```y``` на ```x```), которая лучше всего описывает прямолинейное соотношение между этими двумя линейными переменными. 
 
 ## Простая (парная) линейная регрессия
 
 [Y = a + bx](http://statistica.ru/upload/medialibrary/teoria-lineinoi-regressii/ris1.PNG)
 
 
 ```x``` - независимая переменная, предиктор
 
 
 ```Y``` - зависимая переменная или переменная отклика. Это значение, которое мы ожидаем для ```y``` в среднем, если мы знаем величину ```x```, то это "предсказанное значение ```y```".
 
 
 ```a``` - свободный член (пересечение) линии оценки, значение ```Y```, когда ```x=0```
 
 
 ```b``` - угловой коэффициент, [градиент](http://matica.org.ua/metodichki-i-knigi-po-matematike/metody-optimizatcii-nekrasova-m-g/5-6-proizvodnaia-po-napravleniiu-gradient-linii-urovnia-funktcii) оцененной линии, представляет собой величину, на которую ```Y``` увеличивается в среднем, если ```x``` увеличивается на единицу.
 
 
 ### Метод наименьших квадратов 

Мы выполняем регрессионный анализ, используя выборку наблюдений, где ```a``` и ```b``` – выборочные оценки истинных (генеральных) параметров, ```α``` и ```β``` , которые определяют линию линейной регрессии в популяции (генеральной совокупности).
Наиболее простым методом определения коэффициентов ```a``` и ```b``` является метод наименьших квадратов [(МНК)](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%BD%D0%B0%D0%B8%D0%BC%D0%B5%D0%BD%D1%8C%D1%88%D0%B8%D1%85_%D0%BA%D0%B2%D0%B0%D0%B4%D1%80%D0%B0%D1%82%D0%BE%D0%B2).
Подгонка оценивается, рассматривая остатки (вертикальное расстояние каждой точки от линии, например, остаток = наблюдаемому ```y``` – предсказанный ```y```. [График](http://statistica.ru/upload/medialibrary/teoria-lineinoi-regressii/ris2.PNG)
